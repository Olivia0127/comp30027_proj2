{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# import librart\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import feature_select\n",
    "import feture_process\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use countvectorizer\n",
    "# use countvectorizer\n",
    "# use countvectorizer\n",
    "# use countvectorizer\n",
    "\n",
    "# import data\n",
    "train_df = pd.read_csv(\"./project_data_files/book_rating_train.csv\")\n",
    "test_df = pd.read_csv(\"./project_data_files/book_rating_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Prublisher and Language\n",
    "train_df[\"Language\"], test_df[\"Language\"] = feture_process.docclass_preprocess(train_df[\"Language\"],test_df[\"Language\"],10)\n",
    "train_df[\"Publisher\"], test_df[\"Publisher\"] = feture_process.docclass_preprocess(train_df[\"Publisher\"],test_df[\"Publisher\"],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for Publisher (sklearn)\n",
    "publisher_train_hot = feture_process.process_OneHotEncoder(train_df,\"Publisher\")\n",
    "publisher_test_hot = feture_process.process_OneHotEncoder(test_df,\"Publisher\")\n",
    "train_df = pd.concat([train_df, publisher_train_hot], axis=1)\n",
    "test_df = pd.concat([test_df, publisher_test_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for Language (sklearn)\n",
    "language_train_hot = feture_process.process_OneHotEncoder(train_df,\"Language\")\n",
    "language_test_hot = feture_process.process_OneHotEncoder(test_df,\"Language\")\n",
    "train_df = pd.concat([train_df, language_train_hot], axis=1)\n",
    "test_df = pd.concat([test_df, language_test_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for label (sklearn)\n",
    "label_train_hot = feture_process.process_OneHotEncoder(train_df,\"rating_label\")\n",
    "train_df = pd.concat([train_df, label_train_hot], axis=1)\n",
    "train_df = train_df.drop(columns=[\"rating_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_name_countvectorizer\n",
    "train_name_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "train_name_dic = train_name_countvectorizer.vocabulary_\n",
    "\n",
    "# train_authors_countvectorizer\n",
    "train_authors_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_authors_countvectorizer.pkl\", \"rb\"))\n",
    "train_authors_dic = train_authors_countvectorizer.vocabulary_\n",
    "\n",
    "# train_desc_countvectorizer\n",
    "train_desc_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_desc_countvectorizer.pkl\", \"rb\"))\n",
    "train_desc__dic = train_desc_countvectorizer.vocabulary_\n",
    "\n",
    "# process vector features\n",
    "train_name_features = train_name_countvectorizer.transform(train_df['Name'])\n",
    "train_authors_features = train_authors_countvectorizer.transform(train_df['Authors'])\n",
    "train_desc_features = train_desc_countvectorizer.transform(train_df['Description'])\n",
    "other_features_df = train_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language','rating_label_3.0','rating_label_4.0','rating_label_5.0'])\n",
    "train_label = train_df[['rating_label_3.0','rating_label_4.0','rating_label_5.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use dense matrix\n",
    "train_features = other_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sparse features\n",
    "sparse_features = hstack([train_name_features, train_authors_features, train_desc_features])\n",
    "# new train features\n",
    "dense_features = csr_matrix(other_features_df.values)\n",
    "train_features = hstack([sparse_features, dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test features\n",
    "test_name_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_name_vec.npz')\n",
    "test_authors_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_authors_vec.npz')\n",
    "test_desc_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_desc_vec.npz')\n",
    "test_other_features_df = test_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use dense matrix\n",
    "test_features = test_other_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sparse_features = hstack([test_name_features, test_authors_features, test_desc_features])\n",
    "new_dense_features = csr_matrix(test_other_features_df.values)\n",
    "test_features = hstack([test_sparse_features, new_dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5766, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 12, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 6, 32)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4, 64)             6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 2, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,787\n",
      "Trainable params: 14,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (train_features.shape[1], 1)  # 100个特征，每个特征1个通道\n",
    "num_classes = train_label.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 添加第一个卷积层\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# 添加第二个卷积层\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# 将卷积结果扁平化，以便连接到全连接层\n",
    "model.add(Flatten())\n",
    "\n",
    "# 添加全连接层\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# 添加输出层，使用softmax激活函数进行多分类\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 编译模型，使用分类交叉熵损失函数和Adam优化器\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 打印模型概览\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_label, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 1.2362 - accuracy: 0.6176 - val_loss: 0.8677 - val_accuracy: 0.7063\n",
      "Epoch 2/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.8588 - accuracy: 0.6502 - val_loss: 0.7402 - val_accuracy: 0.7070\n",
      "Epoch 3/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.8188 - accuracy: 0.6557 - val_loss: 0.7584 - val_accuracy: 0.7073\n",
      "Epoch 4/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7835 - accuracy: 0.6703 - val_loss: 0.8118 - val_accuracy: 0.7078\n",
      "Epoch 5/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7701 - accuracy: 0.6796 - val_loss: 0.7108 - val_accuracy: 0.7067\n",
      "Epoch 6/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7376 - accuracy: 0.6927 - val_loss: 0.7603 - val_accuracy: 0.7079\n",
      "Epoch 7/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7454 - accuracy: 0.6933 - val_loss: 0.7471 - val_accuracy: 0.7072\n",
      "Epoch 8/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7290 - accuracy: 0.7001 - val_loss: 0.7356 - val_accuracy: 0.7072\n",
      "Epoch 9/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7319 - accuracy: 0.7011 - val_loss: 0.7244 - val_accuracy: 0.7075\n",
      "Epoch 10/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7288 - accuracy: 0.7007 - val_loss: 0.7133 - val_accuracy: 0.7072\n",
      "Epoch 11/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7248 - accuracy: 0.7008 - val_loss: 0.7116 - val_accuracy: 0.7072\n",
      "Epoch 12/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7234 - accuracy: 0.7007 - val_loss: 0.7307 - val_accuracy: 0.7072\n",
      "Epoch 13/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7253 - accuracy: 0.7006 - val_loss: 0.7353 - val_accuracy: 0.7072\n",
      "Epoch 14/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7260 - accuracy: 0.7009 - val_loss: 0.7253 - val_accuracy: 0.7083\n",
      "Epoch 15/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7251 - accuracy: 0.7009 - val_loss: 0.7870 - val_accuracy: 0.7072\n",
      "Epoch 16/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7235 - accuracy: 0.7006 - val_loss: 0.7772 - val_accuracy: 0.7083\n",
      "Epoch 17/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7200 - accuracy: 0.7013 - val_loss: 0.7915 - val_accuracy: 0.7085\n",
      "Epoch 18/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7193 - accuracy: 0.7008 - val_loss: 0.7732 - val_accuracy: 0.7072\n",
      "Epoch 19/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7178 - accuracy: 0.7011 - val_loss: 0.7731 - val_accuracy: 0.7072\n",
      "Epoch 20/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7189 - accuracy: 0.7010 - val_loss: 0.7596 - val_accuracy: 0.7082\n",
      "Epoch 21/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7216 - accuracy: 0.7002 - val_loss: 0.7243 - val_accuracy: 0.7081\n",
      "Epoch 22/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7181 - accuracy: 0.7006 - val_loss: 0.7352 - val_accuracy: 0.7079\n",
      "Epoch 23/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7178 - accuracy: 0.7012 - val_loss: 0.7299 - val_accuracy: 0.7083\n",
      "Epoch 24/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7206 - accuracy: 0.7007 - val_loss: 0.7066 - val_accuracy: 0.7081\n",
      "Epoch 25/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7185 - accuracy: 0.7008 - val_loss: 0.7155 - val_accuracy: 0.7082\n",
      "Epoch 26/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7191 - accuracy: 0.7006 - val_loss: 0.7076 - val_accuracy: 0.7083\n",
      "Epoch 27/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7180 - accuracy: 0.7008 - val_loss: 0.7184 - val_accuracy: 0.7072\n",
      "Epoch 28/50\n",
      "505/505 [==============================] - 0s 980us/step - loss: 0.7185 - accuracy: 0.7009 - val_loss: 0.7177 - val_accuracy: 0.7072\n",
      "Epoch 29/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7175 - accuracy: 0.7008 - val_loss: 0.7270 - val_accuracy: 0.7072\n",
      "Epoch 30/50\n",
      "505/505 [==============================] - 0s 974us/step - loss: 0.7172 - accuracy: 0.7013 - val_loss: 0.7135 - val_accuracy: 0.7083\n",
      "Epoch 31/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7170 - accuracy: 0.7011 - val_loss: 0.7165 - val_accuracy: 0.7082\n",
      "Epoch 32/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7162 - accuracy: 0.7012 - val_loss: 0.7118 - val_accuracy: 0.7072\n",
      "Epoch 33/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7195 - accuracy: 0.7009 - val_loss: 0.7078 - val_accuracy: 0.7083\n",
      "Epoch 34/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7249 - accuracy: 0.7009 - val_loss: 0.7431 - val_accuracy: 0.7081\n",
      "Epoch 35/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7149 - accuracy: 0.7011 - val_loss: 0.7674 - val_accuracy: 0.7072\n",
      "Epoch 36/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7162 - accuracy: 0.7013 - val_loss: 0.7524 - val_accuracy: 0.7083\n",
      "Epoch 37/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7155 - accuracy: 0.7011 - val_loss: 0.7744 - val_accuracy: 0.7072\n",
      "Epoch 38/50\n",
      "505/505 [==============================] - 0s 968us/step - loss: 0.7165 - accuracy: 0.7006 - val_loss: 0.7320 - val_accuracy: 0.7083\n",
      "Epoch 39/50\n",
      "505/505 [==============================] - 0s 958us/step - loss: 0.7159 - accuracy: 0.7010 - val_loss: 0.7838 - val_accuracy: 0.7072\n",
      "Epoch 40/50\n",
      "505/505 [==============================] - 0s 960us/step - loss: 0.7154 - accuracy: 0.7014 - val_loss: 0.7521 - val_accuracy: 0.7083\n",
      "Epoch 41/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7157 - accuracy: 0.7010 - val_loss: 0.7387 - val_accuracy: 0.7079\n",
      "Epoch 42/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7171 - accuracy: 0.7008 - val_loss: 0.7375 - val_accuracy: 0.7082\n",
      "Epoch 43/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7152 - accuracy: 0.7007 - val_loss: 0.7468 - val_accuracy: 0.7072\n",
      "Epoch 44/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7211 - accuracy: 0.7011 - val_loss: 0.7587 - val_accuracy: 0.7081\n",
      "Epoch 45/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7147 - accuracy: 0.7011 - val_loss: 0.7656 - val_accuracy: 0.7075\n",
      "Epoch 46/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7153 - accuracy: 0.7009 - val_loss: 0.7416 - val_accuracy: 0.7081\n",
      "Epoch 47/50\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.7153 - accuracy: 0.7013 - val_loss: 0.7238 - val_accuracy: 0.7082\n",
      "Epoch 48/50\n",
      "505/505 [==============================] - 0s 972us/step - loss: 0.7151 - accuracy: 0.7013 - val_loss: 0.7495 - val_accuracy: 0.7075\n",
      "Epoch 49/50\n",
      "505/505 [==============================] - 0s 980us/step - loss: 0.7139 - accuracy: 0.7014 - val_loss: 0.7571 - val_accuracy: 0.7072\n",
      "Epoch 50/50\n",
      "505/505 [==============================] - 0s 966us/step - loss: 0.7154 - accuracy: 0.7010 - val_loss: 0.7371 - val_accuracy: 0.7083\n"
     ]
    }
   ],
   "source": [
    "cnn = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
