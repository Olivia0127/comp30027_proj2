{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import librart\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import feature_select\n",
    "import feture_process\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./project_data_files/book_rating_train.csv\")\n",
    "test_df = pd.read_csv(\"./project_data_files/book_rating_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot code Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Prublisher and Language\n",
    "train_df[\"Language\"], test_df[\"Language\"] = feture_process.docclass_preprocess(train_df[\"Language\"],test_df[\"Language\"],10)\n",
    "train_df[\"Publisher\"], test_df[\"Publisher\"] = feture_process.docclass_preprocess(train_df[\"Publisher\"],test_df[\"Publisher\"],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for Publisher (sklearn)\n",
    "publisher_train_hot = feture_process.process_OneHotEncoder(train_df,\"Publisher\")\n",
    "publisher_test_hot = feture_process.process_OneHotEncoder(test_df,\"Publisher\")\n",
    "train_df = pd.concat([train_df, publisher_train_hot], axis=1)\n",
    "test_df = pd.concat([test_df, publisher_test_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for Language (sklearn)\n",
    "language_train_hot = feture_process.process_OneHotEncoder(train_df,\"Language\")\n",
    "language_test_hot = feture_process.process_OneHotEncoder(test_df,\"Language\")\n",
    "train_df = pd.concat([train_df, language_train_hot], axis=1)\n",
    "test_df = pd.concat([test_df, language_test_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use oneHotCode for label (sklearn)\n",
    "label_train_hot = feture_process.process_OneHotEncoder(train_df,\"rating_label\")\n",
    "train_df = pd.concat([train_df, label_train_hot], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_name_countvectorizer\n",
    "train_name_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n",
    "train_name_dic = train_name_countvectorizer.vocabulary_\n",
    "\n",
    "# train_authors_countvectorizer\n",
    "train_authors_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_authors_countvectorizer.pkl\", \"rb\"))\n",
    "train_authors_dic = train_authors_countvectorizer.vocabulary_\n",
    "\n",
    "# train_desc_countvectorizer\n",
    "train_desc_countvectorizer = pickle.load(open(\"./project_data_files/book_text_features_countvec/train_desc_countvectorizer.pkl\", \"rb\"))\n",
    "train_desc__dic = train_desc_countvectorizer.vocabulary_\n",
    "\n",
    "# process vector features\n",
    "train_name_features = train_name_countvectorizer.transform(train_df['Name'])\n",
    "train_authors_features = train_authors_countvectorizer.transform(train_df['Authors'])\n",
    "train_desc_features = train_desc_countvectorizer.transform(train_df['Description'])\n",
    "other_features_df = train_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language','rating_label_3.0', 'rating_label_4.0', 'rating_label_5.0', 'rating_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sparse features\n",
    "sparse_features = hstack([train_name_features, train_authors_features, train_desc_features])\n",
    "# new train features\n",
    "dense_features = csr_matrix(other_features_df.values)\n",
    "train_features = hstack([sparse_features, dense_features])\n",
    "\n",
    "\n",
    "# process test features\n",
    "test_name_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_name_vec.npz')\n",
    "test_authors_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_authors_vec.npz')\n",
    "test_desc_features = scipy.sparse.load_npz('./project_data_files/book_text_features_countvec/test_desc_vec.npz')\n",
    "test_other_features_df = test_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language'])\n",
    "\n",
    "test_sparse_features = hstack([test_name_features, test_authors_features, test_desc_features])\n",
    "new_dense_features = csr_matrix(test_other_features_df.values)\n",
    "test_features = hstack([test_sparse_features, new_dense_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use doc2vec\n",
    "# use doc2vec\n",
    "# use doc2vec\n",
    "# use doc2vec\n",
    "\n",
    "# process vector features\n",
    "train_name_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_authors_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_desc_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "other_features_df = train_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language', 'rating_label', 'rating_label_3.0', 'rating_label_4.0', 'rating_label_5.0'])\n",
    "train_features = pd.concat([train_name_features, train_authors_features, train_desc_features, other_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use doc2vec\n",
    "# use doc2vec\n",
    "# use doc2vec\n",
    "# use doc2vec\n",
    "\n",
    "# process test features\n",
    "test_name_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_authors_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_desc_features = pd.read_csv(r\"./project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_other_features_df = test_df.drop(columns=['Name', 'Authors', 'Description', 'Publisher', 'Language'])\n",
    "test_features = pd.concat([test_name_features, test_authors_features, test_desc_features, test_other_features_df], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,selected_features_test = train_features,test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,selected_features_test = feature_select.MI(train_df,train_features,test_features,12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,selected_features_test = feature_select.chi_square(train_df,train_features,test_features,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,selected_features_test = feature_select.variance_ratio(train_df,train_features,test_features,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = load_npz(\"selected_features_label_one_hot_12000.npz\")\n",
    "selected_features_test = load_npz(\"selected_features_test_label_one_hot_12000.npz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper parameter choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(input_dim, hidden_layers=1, hidden_units=64, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=input_dim, activation=activation,kernel_regularizer=l2(0.0001)))\n",
    "\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(hidden_units, activation=activation))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "input_dim = selected_features.shape[1] \n",
    "mlp = KerasClassifier(build_fn=create_mlp, input_dim=input_dim, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'hidden_units': [32, 64, 128],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100],\n",
    "}\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('gridsearch', GridSearchCV(mlp, param_grid, cv=3, n_jobs=-1, verbose=1)),\n",
    "])\n",
    "X_train, X_val, y_train, y_val = train_test_split(selected_features, train_df[\"rating_label\"], test_size=0.3, random_state=66)\n",
    "X_train = X_train.toarray()\n",
    "X_val = X_val.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pipeline.named_steps['gridsearch'].best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2\n",
    "X_train, X_val, y_train, y_val = train_test_split(selected_features, label_train_hot, test_size=0.3, random_state=66)\n",
    "\n",
    "input_dim = selected_features.shape[1] \n",
    "num_classes = label_train_hot.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='leaky_relu', input_shape=(input_dim,)\n",
    "                , kernel_regularizer=l2(0.0001)\n",
    "                ))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()\n",
    "X_val = X_val.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {\n",
    "    (1, 0, 0): 2.0,\n",
    "    (0, 1, 0): 1.0,\n",
    "    (0, 0, 1): 3.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = np.array(y_train)\n",
    "sample_weights = np.array([class_weights[tuple(y)] for y in y_train_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "unique_classes = np.unique(y_train)\n",
    "sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val)\n",
    "                , sample_weight=sample_weights\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mlp.history['loss'], label='Training Loss')\n",
    "plt.plot(mlp.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Learning Curve')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mlp.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(mlp.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Learning Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for variance_ratio slection\n",
    "import tensorflow as tf\n",
    "coo_matrix = selected_features_test.tocoo()\n",
    "indices = np.column_stack((coo_matrix.row, coo_matrix.col))\n",
    "values = coo_matrix.data\n",
    "shape = coo_matrix.shape\n",
    "\n",
    "sparse_tensor = tf.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "ordered_sparse_tensor = tf.sparse.reorder(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_prob = model.predict(ordered_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "prediction_index = np.argmax(prediction_prob, axis=1)\n",
    "ratings = np.array([3.0, 4.0, 5.0])\n",
    "prediction = ratings[prediction_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'rating_label': prediction})\n",
    "output_df.index += 1\n",
    "output_df.index.name = 'id'\n",
    "output_df.to_csv('./predictions.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, train_df[\"rating_label\"], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, \n",
    "                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                    max_iter=200, shuffle=True, random_state=None)\n",
    "\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=mlp, n_estimators=10, max_samples=0.8, max_features=1.0, \n",
    "                            bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=42)\n",
    "\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging.predict(selected_features_test)\n",
    "output_df = pd.DataFrame({'rating_label': y_pred})\n",
    "output_df.index += 1\n",
    "output_df.index.name = 'id'\n",
    "output_df.to_csv('./predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
